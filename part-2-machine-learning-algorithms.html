<!DOCTYPE html>
<html>
<head>
<title>part-2-machine-learning-algorithms.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>
<h1 id="part-2-machine-learning-algorithms">Part 2: Machine Learning Algorithms</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li>
<p><a href="#ml-problem-settings">ML Problem Settings</a></p>
<ul>
<li><a href="#basic-assumptions-and-definitions">Basic Assumptions and Definitions</a></li>
<li><a href="#optimization-gradient-descent">Optimization (Gradient Descent)</a></li>
</ul>
</li>
<li>
<p><a href="#deterministic--probabilistic-problems">Deterministic &amp; Probabilistic Problems</a></p>
</li>
<li>
<p><a href="#paramertric-vs-non-parametric-models">Paramertric vs. Nonparametric Models</a></p>
<ul>
<li><a href="#parametric-models">Parametric Models</a></li>
<li><a href="#nonparametric-models">Nonparametric Models</a></li>
</ul>
</li>
<li>
<p><a href="#diagnose-ml-model">Diagnose ML model</a></p>
</li>
<li>
<p><a href="#supervised-learning">Supervised Learning</a></p>
<ol>
<li><a href="#decision-tree">Decision Tree (DT)</a>
<ul>
<li><a href="#random-forest-rf">Random Forest (RF)</a></li>
<li><a href="#boosting-regression-trees-gradient-boosting">Boosting Regression Trees (Gradient Boosting)</a></li>
<li><a href="#boosting-classification-trees-adaboost">Boosting Classification Trees (AdaBoost)</a></li>
</ul>
</li>
<li><a href="#knn-k-nearest-neighbors-classification">KNN (K-Nearest Neighbors) Classification</a></li>
<li><a href="#perceptron">Perceptron</a></li>
<li><a href="#linear-regression">Linear Regression</a>
<ul>
<li><a href="#regularization">Regularization</a>
<ul>
<li>Ridge Regression (L2 Regularization)</li>
<li>Lasso Regression (L1 Regularization)</li>
</ul>
</li>
</ul>
</li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#neural-network">Neural Network</a></li>
<li><a href="#naive-bayes">Naive Bayes</a></li>
<li><a href="#support-vector-machine-svm">Support Vector Machine (SVM)</a></li>
<li><a href="#k-means-clustering">K-Means Clustering</a></li>
<li><a href="#time-series-forecasting">Time Series Forecasting</a></li>
</ol>
</li>
</ul>
<hr>
<h2 id="ml-problem-settings">ML Problem Settings</h2>
<h3 id="basic-assumptions-and-definitions">Basic Assumptions and Definitions</h3>
<ol>
<li>
<p><strong>Probabilistic Model</strong>: The dataset is considered to be generated from an unknown probability distribution of input-output pairs.</p>
</li>
<li>
<p><strong>Definition of ML</strong>:</p>
<p>Compute a <strong>function</strong> $f$ that has low expected error $\epsilon$ over <strong>sample</strong> $D$ (from unknown distribution) with respect to <strong>loss function</strong> $L$</p>
<ul>
<li><strong>Key Assumption</strong>: The target function is unknown</li>
<li><strong>Model</strong>: Defines the hypothesis space for the learning algorithm to search</li>
<li><strong>Model Parameters</strong>: Numerical values or structures used to define hypotheses</li>
<li><strong>Learning Algorithm</strong>: Searches the hypothesis space to minimize loss</li>
<li><strong>Hyperparameters</strong>: Tunable model aspects not selected by the learning algorithm</li>
<li><strong>Inductive Bias</strong>: Assumptions used by the learner to predict outputs of given inputs for unseen data</li>
</ul>
</li>
<li>
<p><strong>Structure of ML Algorithm</strong>:</p>
<ul>
<li><strong>Inputs (features)</strong>: $x^i \in R^n$, $ i = 1, \ldots, m$</li>
<li><strong>Outputs (targets)</strong>: $y^i \in Y, $ $i = 1, \ldots, m$</li>
<li><strong>Parameters</strong>: $ \theta \in R^d$</li>
<li><strong>Hypothesis Function</strong>: $ h_\theta: R^n \to \hat{Y}$</li>
<li><strong>Loss Function</strong>: $ \ell: \hat{Y} \times Y \to R_+$</li>
<li><strong>Canonical Optimization Problem</strong>: $\min_{\theta \in \Theta} \frac{1}{m} \sum_{i=1}^m \ell(h_\theta(x^i), y^i)$</li>
</ul>
</li>
</ol>
<h3 id="optimization-gradient-descent">Optimization (Gradient Descent)</h3>
<p><strong>Gradient Descent</strong> is an iterative optimization algorithm used to find the minimum of a function</p>
<img src="./res/gradient-descent.png" alt="" width="500">
<ul>
<li>
<p><strong>Algorithm</strong>: Repeatedly update weights by calculating the <strong>negative partial derivatives</strong> for each coordinate</p>
<ul>
<li><strong>Starting point</strong> ($w_{\text{old}}$): random or zero</li>
<li><strong>Stopping criterion</strong>: Stop when the gradient ($\frac{\partial J}{\partial w}$) becomes close to zero or when the change in $J(w)$ is below a predefined threshold</li>
<li><strong>Learning rate</strong> ($\alpha$): Step size</li>
</ul>
</li>
<li>
<p>Useful in cases where <strong>closed-form solutions</strong> are <em>unavailable</em> or <em>infeasible</em></p>
<ul>
<li>Total complexity for OLS: $O(M^2N + M^{2.373})$, where $M$ is the number of features and $N$ is the number of data points</li>
</ul>
</li>
<li>
<p><strong>Pros</strong>:</p>
<ul>
<li>Simple to implement</li>
<li>Widely applicable for a variety of optimization problems</li>
</ul>
</li>
<li>
<p><strong>Cons</strong>:</p>
<ul>
<li>Slow convergence on large datasets</li>
<li>May find local minima instead of global minima</li>
<li>May not be suitable for non-convex problems</li>
</ul>
</li>
<li>
<p><strong>GD Variants</strong></p>
  <img src="./res/gd-variants.png" alt="" width="500">
<table>
<thead>
<tr>
<th>Method</th>
<th>Data Processed At a Time</th>
<th>Gradient Quality</th>
<th>Speed</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch GD</strong></td>
<td>Entire dataset</td>
<td>High (accurate gradients)</td>
<td>Slowest</td>
<td>Very Stable</td>
</tr>
<tr>
<td><strong>Mini-Batch GD</strong></td>
<td>Batch of data points</td>
<td>Balanced</td>
<td>Balanced</td>
<td>Balanced</td>
</tr>
<tr>
<td><strong>Stochastic GD (SGD)</strong></td>
<td>Single data point</td>
<td>Noisy (approximate gradients)</td>
<td>Fastest</td>
<td>Less Stable</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>Appendix</strong>: Complexity of matrix operations:</p>
<ul>
<li>Inner product $x^T y$: $O(n)$</li>
<li>Matrix-vector product $Ax$: $O(n^2)$</li>
<li>Matrix-matrix product $AB$: $O(n^3)$</li>
<li>Matrix inverse $A^{-1}$: $O(n^3)$</li>
</ul>
</li>
</ul>
<h2 id="deterministic--probabilistic-problems">Deterministic &amp; Probabilistic Problems</h2>
<p>[TODO]</p>
<h2 id="paramertric-vs-nonparametric-models">Paramertric vs. Nonparametric Models</h2>
<h3 id="parametric-models">Parametric Models</h3>
<ul>
<li>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Assume a specific distribution for the data</li>
<li>Have a fixed number of parameters</li>
<li>Complexity of the model is <strong>bounded</strong>, regardless of the amount of data</li>
</ul>
</li>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Linear SVM</li>
<li>K-Means Clustering</li>
<li>PCA</li>
</ul>
</li>
</ul>
<h3 id="nonparametric-models">Nonparametric Models</h3>
<ul>
<li>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Do not make strong assumptions about the underlying function</li>
<li>The number of parameters grows with the size of the training data</li>
<li>More flexible but <strong>require more training data</strong></li>
</ul>
</li>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li>K-Nearest Neighbor (k-NN)</li>
<li>Decision Trees</li>
<li>RBF Kernel SVMs</li>
<li>Gaussian Processes (GPs)</li>
</ul>
</li>
</ul>
<h2 id="diagnose-ml-model">Diagnose ML model</h2>
<p>[TODO]</p>
<h2 id="supervised-learning">Supervised Learning</h2>
<table>
<thead>
<tr>
<th><strong>Algorithm</strong></th>
<th><strong>Hypothesis</strong></th>
<th><strong>Loss Function</strong></th>
<th><strong>Optimization Method</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Linear Regression</strong></td>
<td>Linear</td>
<td>Any regression loss</td>
<td>Any</td>
</tr>
<tr>
<td><strong>SVM</strong></td>
<td>Linear or kernel</td>
<td>Hinge loss</td>
<td>Any</td>
</tr>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>(Usually) linear</td>
<td>Logistic loss</td>
<td>Any</td>
</tr>
<tr>
<td><strong>NN (Neural Networks)</strong></td>
<td>Composed non-linear function</td>
<td>Any</td>
<td>(Usually) gradient descent</td>
</tr>
<tr>
<td><strong>DT (Decision Tree)</strong></td>
<td>Axis-aligned halfplanes</td>
<td>Log probability under Bernoulli model</td>
<td>Greedy search</td>
</tr>
<tr>
<td><strong>Naive Bayes</strong></td>
<td>Linear</td>
<td>Joint probability of data and output</td>
<td>Analytic solution</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>Ensemble of other models</td>
<td>Any</td>
<td>Gradient descent</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="decision-tree-dt">Decision Tree (DT)</h2>
<h3 id="pros">Pros</h3>
<ul>
<li>Easy to interpret and visualize</li>
<li>Computationally efficient in both time and memory</li>
<li>Widely applicable to classification, regression, and density estimation</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<ol>
<li>Search for the best attribute (<strong>splitting criterion</strong>) to split the data</li>
<li>Use the chosen attribute to split data into subsets</li>
<li>Repeat steps 1 and 2 for each subset until <strong>stopping criteria</strong> are met</li>
<li>Optionally <strong>prune</strong> the tree to prevent overfitting</li>
</ol>
<ul>
<li><strong>Complexity</strong>:
<ul>
<li>Training: $O(MN \log N)$</li>
<li>Prediction: $O(2^M \log N)$</li>
</ul>
</li>
</ul>
<h3 id="splitting-criteria-in-decision-trees">Splitting Criteria in Decision Trees</h3>
<p>Splitting criteria determine the best way to split data at each node of the tree.</p>
<ol>
<li>
<p><strong>Information Gain</strong></p>
<ul>
<li>
<p>Measures the reduction in entropy (uncertainty)</p>
</li>
<li>
<p><strong>Goal</strong>: Choose the attribute that <em>maximizes</em> the information gain after the split</p>
  <div>
  $$
  IG(S, A) = Entropy(S) - \sum_{i} \frac{|S_i|}{|S|} \cdot Entropy(S_i)
  $$
  </div>
<p>where:</p>
<ul>
<li>$S$: Original dataset</li>
<li>$A$: Attribute being split</li>
<li>$S_i$: Subset created by splitting $S$ on $A$</li>
</ul>
</li>
<li>
<p><strong>Entropy</strong> (for classification problems):</p>
  <div>
  $$
  Entropy(S) = -\sum_{j} p_j \log_2(p_j)
  $$
  </div>
<p>where $p_j$ is the proportion of class $j$ in the dataset</p>
</li>
<li>
<p><strong>(+)</strong>: Works particularly well for multi-class problems</p>
</li>
</ul>
</li>
<li>
<p><strong>Gini Index</strong></p>
<ul>
<li>
<p>Measures the impurity of a dataset</p>
</li>
<li>
<p><strong>Goal</strong>: Choose the attribute that <em>minimizes</em> the Gini Index after the split</p>
</li>
<li>
<p><strong>Formula</strong>:</p>
  <div>
  $$
  Gini(S) = 1 - \sum_{j} p_j^2
  $$
  </div>
<p>where $p_j$ is the proportion of class $j$ in the dataset</p>
</li>
<li>
<p><strong>(+)</strong>: Computationally efficient and widely used in practical implementations, especially in CART (Classification and Regression Trees)</p>
</li>
</ul>
</li>
<li>
<p><strong>Error Rate</strong></p>
<ul>
<li>
<p>Measures the misclassification error of a split</p>
</li>
<li>
<p><strong>Goal</strong>: Choose the attribute that <em>minimizes</em> the error rate after the split</p>
</li>
<li>
<p><strong>Formula</strong>:</p>
  <div>
  $$
  Error\ Rate(S) = 1 - \max(p_j)
  $$
  </div>
<p>where $p_j$ is the proportion of the majority class in the dataset</p>
</li>
<li>
<p><strong>(-)</strong>: Less sensitive to changes in data distribution compared to Information Gain or Gini Index and rarely used in practice</p>
</li>
</ul>
</li>
</ol>
<h3 id="stopping-criteria">Stopping Criteria</h3>
<ul>
<li>Splitting criterion falls below a threshold</li>
<li>Maximum tree depth is achieved</li>
<li>Split is not statistically significant</li>
<li>Full tree is grown, then pruned</li>
</ul>
<h3 id="pruning">Pruning</h3>
<ol>
<li>Prepare a validation set to evaluate the impact of pruning</li>
<li>Greedily remove nodes that improve validation accuracy</li>
<li>Stop pruning when further pruning is detrimental</li>
</ol>
<h3 id="inductive-bias-in-decision-trees">Inductive Bias in Decision Trees</h3>
<ul>
<li><strong>ID3 Algorithm</strong> (Greedy Search):
<ul>
<li><strong>Smallest Tree</strong>: Assumes that smaller trees are better generalizations and less likely to overfit</li>
<li><strong>High Mutual Information</strong>: Favors splitting on attributes that provide the most information gain (reduce uncertainty the most) near the root of the tree, ensuring that the most informative decisions are made early</li>
</ul>
</li>
</ul>
<hr>
<h2 id="random-forest-rf">Random Forest (RF)</h2>
<p>Random Forest is an <strong>ensemble method</strong> that combines multiple decision trees
using <strong>bagging</strong> and <strong>feature randomness</strong> to create a forest of uncorrelated trees</p>
<h3 id="random-forest-process">Random Forest Process</h3>
<ul>
<li><strong>Bootstrap Sampling (Bagging)</strong>:
<ul>
<li>Draw a bootstrap dataset $D_b$ of size $n$ from training data $D$</li>
<li>Grow a decision tree $T_b$ using $D_b$ by:
<ol>
<li>Selecting $p$ (typically $p \approx \sqrt{d}$) random features from $d$
<ul>
<li>Equivalent to vanilla bagging when $p = d$</li>
</ul>
</li>
<li>Choosing the best feature among $p$ to split</li>
<li>Recursively splitting until the stopping criterion is met (e.g., minimum leaf size)</li>
</ol>
</li>
</ul>
</li>
<li><strong>Output</strong>: An ensemble of trees ${T_1, T_2, \dots, T_B}$</li>
</ul>
<h3 id="out-of-bag-oob-score">Out-of-Bag (OOB) Score</h3>
<ul>
<li><strong>Bootstrap Sampling</strong>: Each decision tree is trained on a randomly sampled subset of the training data (with replacement)</li>
<li>On average, 63% of the original data is included in each bootstrap sample for training, while 37% of the data is left out as <strong>out-of-bag (OOB) samples</strong></li>
<li>OOB score is calculated as:
<ul>
<li>the mean squared error (MSE) (for regression)</li>
<li>the accuracy (for classification)</li>
</ul>
</li>
<li>OOB score serves as an <strong>unbiased estimate</strong> of the Random Forest's generalization error without the need for additional validation datasets</li>
</ul>
<h3 id="interpretability-and-feature-importance">Interpretability and Feature Importance</h3>
<ul>
<li><strong>Trade-off</strong>: Bagging improves prediction accuracy but reduces interpretability</li>
<li><strong>Feature Importance</strong>
<ul>
<li>For regression trees: Calculated as the <strong>total reduction in RSS</strong> (Residual Sum of Squares) due to splits involving a feature, averaged across all $B$ trees</li>
<li>For classification trees: Measured by the <strong>total decrease in impurity</strong> (e.g., Gini index) caused by splits involving a feature, averaged over all $B$ trees</li>
</ul>
</li>
<li>Features are ranked by the average contribution to the measures, where higher values indicate greater importance</li>
</ul>
<h3 id="advantages-of-random-forest">Advantages of Random Forest</h3>
<ol>
<li>Handles missing values and outliers</li>
<li>Less prone to overfitting; Low bias and moderate variance</li>
<li>Generalizes to high-dimensional data and correlated features</li>
<li>Robust to data variations and non-linear patterns</li>
<li>Can be trained in parallel for efficiency</li>
<li>Does not require cross-validation to estimate error</li>
</ol>
<hr>
<h2 id="boosting-regression-trees-gradient-boosting">Boosting Regression Trees (Gradient Boosting)</h2>
<ul>
<li>Boosting mirrors human learning by iteratively improving performance on challenging examples</li>
<li>Examples: XGBoost, LightGBM, CatBoost</li>
</ul>
<h3 id="steps-to-build-boosted-regression-trees">Steps to Build Boosted Regression Trees</h3>
<ol>
<li>Initialize:
<ul>
<li>$f(x) = 0$ and residuals $r_i = y_i$ for all instances $i$ in the training set $(X, y)$</li>
</ul>
</li>
<li>For each boosting iteration $b = 1, \dots, B$:
<ol>
<li>Fit a regression tree $T_b$ to the training data $(X, r)$
<ul>
<li>The tree predicts $f_b(x)$ for an instance $x$</li>
</ul>
</li>
<li>Update the prediction model:
<ul>
<li>$f(x) \leftarrow f(x) + \lambda f_b(x)$, where $\lambda$ is the learning rate</li>
</ul>
</li>
<li>Update the residuals:
<ul>
<li>$r_i \leftarrow r_i - \lambda f_b(x_i)$</li>
</ul>
</li>
</ol>
</li>
<li>Output the final boosted regression model:
<ul>
<li>$f(x) = \sum_{b=1}^B \lambda f_b(x)$</li>
</ul>
</li>
</ol>
<h3 id="hyperparameters">Hyperparameters</h3>
<ol>
<li>
<p><strong>Number of Trees ($B$):</strong></p>
<ul>
<li>Controls the size of the ensemble</li>
<li><strong>Too large</strong> $B$ may lead to overfitting</li>
</ul>
</li>
<li>
<p><strong>Learning Rate ($\lambda$):</strong></p>
<ul>
<li>Controls how much each tree contributes to the final model</li>
<li><strong>Small $\lambda$</strong> requires more iterations but improves model performance</li>
</ul>
</li>
<li>
<p><strong>Regularization Parameter ($h_\text{max}$ or $n_\text{min}$):</strong></p>
<ul>
<li>Limits tree complexity (e.g., maximum depth or minimum leaf size)</li>
<li>Smaller trees reduce overfitting but may require a larger $B$</li>
</ul>
</li>
</ol>
<p>Practical Tips</p>
<ul>
<li>Hyperparameter tuning often involves cross-validation over:
<ul>
<li>A 3D grid of $B$, $\lambda$, and $h_\text{max}$</li>
<li>Alternatively, fixing $B$ and searching over $\lambda$ and $h_\text{max}$ (2D grid)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="boosting-classification-trees-adaboost">Boosting Classification Trees (AdaBoost)</h2>
<h3 id="adaboost-process">AdaBoost Process</h3>
<ul>
<li>
<p>Initialize the weights $ \beta_i^{(0)} = 1/n $ for training instances $ i = 1, \dots, n $ where $ y_i \in {-1, +1} $</p>
</li>
<li>
<p>For $ b = 1, \dots, B $:</p>
<ol>
<li>Fit a decision tree $ T_b $ to training data using weights $ \beta_i^{(b)} $. Denote the prediction function of $ T_b $ as $ f_b : \mathbf{x} \to {-1, +1} $</li>
<li>Compute training error rate for $ f_b $ on the weighted data: <div>
 $$
 \text{error}_b = \frac{\sum_{i=1}^n \beta_i \cdot \mathbb{I}(y_i \neq f_b(x_i))}{\sum_{i=1}^n \beta_i}
 $$
 </div>
</li>
<li>Compute classifier weight $ \alpha_b $ for $ f_b $: <div>
 $$
 \alpha_b = \frac{1}{2} \log \left( \frac{1 - \text{error}_b}{\text{error}_b} \right)
 $$
 </div>
</li>
<li>Update weights of training instances: <div>
 $$
 \beta_i^{(b)} \gets \frac{\beta_i^{(b-1)} \cdot \exp \left[ -\alpha_b \cdot y_i \cdot f_b(x_i) \right]}{\sum_{i=1}^n \beta_i^{(b-1)} \cdot \exp \left[ -\alpha_b \cdot y_i \cdot f_b(x_i) \right]}, \quad i = 1, \dots, n
 $$
 </div>
</li>
</ol>
</li>
<li>
<p>Output the boosted classification model:</p>
  <div>
  $$
  f(\mathbf{x}) = \text{sign} \left( \sum_{b=1}^B \alpha_b f_b(\mathbf{x}) \right)
  $$
  </div>
</li>
</ul>
<hr>
<h2 id="knn-k-nearest-neighbors-classification">KNN (K-Nearest Neighbors) Classification</h2>
<h3 id="algorithm">Algorithm</h3>
<ol>
<li>
<p><strong>Training</strong>: Store the dataset $D$</p>
<ul>
<li>$O(1)$</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>: Assign the <strong>most common label</strong> (majority vote) of the $K$-nearest points in $D$ to a given input based on the computed distances</p>
<ul>
<li>
<p>$O(MN)$, where $M$ is the number of features and $N$ is the size of the dataset</p>
</li>
<li>
<p><strong>Behavior with $K$</strong>:</p>
<ul>
<li>Small $K$: Model may overfit</li>
<li>Large $K$: Majority vote in $D$ dominates</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="distance-functions">Distance Functions</h3>
<ol>
<li>
<p><strong>Euclidean Distance</strong>:</p>
 <div>
 $$
 g(u, v) = \sqrt{\sum_{m=1}^{M} (u_m - v_m)^2}
 $$
 </div>
</li>
<li>
<p><strong>Manhattan Distance</strong>:</p>
 <div>
 $$
 g(u, v) = \sum_{m=1}^{M} |u_m - v_m|
 $$
 </div>
</li>
</ol>
<h3 id="tuning">Tuning</h3>
<ol>
<li>Use different distance functions</li>
<li>Increase $K$</li>
<li>Remove the farthest point</li>
<li>Apply weighted voting (give closer points higher weight)</li>
</ol>
<h3 id="inductive-bias-in-knn">Inductive Bias in KNN</h3>
<ol>
<li>Assumes that <strong>nearby points</strong> are likely to have <strong>similar labels</strong></li>
<li>Assumes <strong>equal importance</strong> for all features (can degrade performance in high-dimensional datasets and lead to the curse of dimensionality)</li>
</ol>
<hr>
<h2 id="perceptron">Perceptron</h2>
<img src="./res/perceptron.png" alt="" width="500">
<p><strong>Perceptron</strong> is a foundational machine learning model designed for <strong>binary classification</strong> tasks</p>
<ul>
<li>Uses <strong>weights</strong> for features to learn a <strong>linear decision boundary</strong></li>
<li>Features with larger absolute weights have greater influence on prediction (requires feature scaling)</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<ol>
<li>Initialize:
<ul>
<li>Set all weights ($\theta$) to zero</li>
</ul>
</li>
<li>While not converged:
<ul>
<li>For each training example $(x^i, y^i)$:
<ul>
<li>Predict $\hat{y} = h_\theta(x) = \text{sign}(\theta^T x)$</li>
<li>If $\hat{y} \neq y$, update weights:
$\theta \gets \theta + y^i x^i$</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="perceptron-mistake-bound">Perceptron Mistake Bound</h3>
<ul>
<li>If data has a margin $\gamma$ and is within a radius $R$, the Perceptron makes at most $(R / \gamma)^2$ mistakes</li>
<li><strong>Scaling Invariance</strong>: Scaling all inputs does not affect the number of mistakes</li>
</ul>
<h3 id="inductive-bias-in-perceptron">Inductive Bias in Perceptron</h3>
<ul>
<li>Assumes <strong>linear decision boundaries</strong></li>
<li>Prioritizes correcting the <strong>most recent mistakes</strong> (suitable for online learning)</li>
</ul>
<h3 id="extensions">Extensions</h3>
<ol>
<li><strong>Voted Perceptron</strong>:
<ul>
<li>Stores all weight vectors during training; Allows weighted voting</li>
<li>Improves generalization but memory-intensive</li>
</ul>
</li>
<li><strong>Averaged Perceptron</strong>:
<ul>
<li>Uses running averages of weights for memory efficiency</li>
</ul>
</li>
<li><strong>Kernel Perceptron</strong>:
<ul>
<li>Uses the <strong>kernel trick</strong> to handle non-linear decision boundaries</li>
</ul>
</li>
<li><strong>Structured Perceptron</strong>:
<ul>
<li>Designed for tasks with exponentially large output spaces</li>
</ul>
</li>
</ol>
<hr>
<h2 id="linear-regression">Linear Regression</h2>
<ul>
<li>
<p><strong>Linear Regression</strong> models the relationship between variables with an <strong>affine combination</strong> (linear combination where the coefficients (weights) sum to 1)</p>
</li>
<li>
<p>Closed-form solution:</p>
  <div>
  $$ \theta = (X^T X)^{-1} X^T y $$
  </div>
</li>
</ul>
<h3 id="assumptions">Assumptions</h3>
<ol>
<li><strong>Linear Relationship</strong>: Independent variables are assumed to have a linear relationship with the dependent variable</li>
<li><strong>Normal Residuals</strong>: Residuals should follow a normal distribution (check via Q-Q plot)</li>
<li><strong>No Multicollinearity</strong>: Independent variables should not be highly correlated to ensure stable coefficient estimates</li>
<li><strong>Homoscedasticity</strong>: Error variance should be constant across all predictor levels</li>
<li><strong>Independent Residuals</strong>: Observations should be randomly and independently sampled</li>
</ol>
<h3 id="data-preparation">Data Preparation</h3>
<ol>
<li>Address non-linear relationships (e.g., log transformation for an exponential relationship)</li>
<li>Clean data to remove noise</li>
<li>Remove highly correlated variables</li>
<li>Standardize variables for better predictions</li>
</ol>
<h3 id="intercept-term">Intercept Term</h3>
<p>The <strong>intercept term</strong> in a regression model ensures that:</p>
<ol>
<li>
<p><strong>Residuals have zero mean</strong>: The average difference between the observed values and the predicted values is zero, which prevents systematic overestimation or underestimation of the dependent variable</p>
</li>
<li>
<p><strong>Unbiased slope estimates</strong>: The inclusion of an intercept allows the regression line to fit the data accurately, ensuring that the estimated slope coefficients are not distorted by forcing the line to pass through the origin</p>
</li>
</ol>
<h3 id="model-evaluation">Model Evaluation</h3>
<ul>
<li><strong>R-squared/Adjusted R-squared</strong></li>
<li><strong>F-test</strong>
<ul>
<li>Null Hypothese $H_0$: All regression coefficients are equal to zero</li>
</ul>
</li>
<li><strong>RMSE</strong></li>
</ul>
<h3 id="regularization">Regularization</h3>
<p><strong>Regularization</strong> involves adding a <strong>penalty term</strong> to the loss function in regression models to prevent overfitting and enhance generalization</p>
<ol>
<li>
<p><strong>Ridge Regression (L2 Regularization)</strong></p>
<ul>
<li>
<p>Penalizes the sum of squared coefficients ($|\mathbf{w}|_2^2 = \sum_j w_j^2$)</p>
</li>
<li>
<p>Loss function:</p>
  <div>
  $$
  \ell(\mathbf{w}) = \frac{1}{2} \|\mathbf{Xw} - \mathbf{y}\|^2 + \frac{\lambda}{2} \|\mathbf{w}\|_2^2
  $$
  </div>
</li>
<li>
<p>Closed-form solution:</p>
  <div>
  $$
  \mathbf{w} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}
  $$
  </div>
</li>
</ul>
</li>
<li>
<p><strong>Lasso Regression (L1 Regularization)</strong></p>
<ul>
<li>Penalizes the absolute sum of coefficients ($|\mathbf{w}|_1 = \sum_j |w_j|$)</li>
<li>Encourages sparsity by driving some coefficients to zero, enabling feature selection</li>
<li>No closed-form solution due to the non-differentiability of $|\mathbf{w}|_1$ at $w_j = 0$</li>
<li>Requires <strong>subgradient optimization</strong> techniques for implementation</li>
</ul>
</li>
</ol>
<p>Practical Tips</p>
<ul>
<li>Proper selection of $\lambda$ balances bias and variance, optimizing model performance
<ul>
<li><strong>$\lambda = 0$</strong>: No regularization, reduces to standard least squares regression, might lead to overfitting</li>
<li><strong>$\lambda \to \infty$</strong>: Coefficients approach zero, might lead to underfitting</li>
</ul>
</li>
</ul>
<hr>
<h2 id="logistic-regression">Logistic Regression</h2>
<p><strong>Logistic Regression</strong> predicts the probability of a <strong>binary outcome</strong> using a linear combination of independent variables transformed by a <strong>logit function</strong></p>
<ul>
<li>Commonly known as the <strong>logit model</strong>, a probabilistic classifier optimized through <strong>MLE</strong></li>
<li>Shares similarities with SVMs in separating hyperplanes, but differs in loss function</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<ul>
<li>
<p><strong>Hypothesis Function</strong>:</p>
  <div>
  $$
  p(y=1|x) = \frac{1}{1 + e^{-\theta^T x}}
  $$
  </div>
<ul>
<li>Models conditional probability using <strong>logits</strong> ($\theta^T x$)</li>
</ul>
</li>
<li>
<p><strong>Loss Function</strong>:</p>
<ul>
<li><strong>Logistic Loss (log-loss)</strong>: $\log(1 + e^{-h_\theta(x) \cdot y})$</li>
<li>Optimized using <strong>gradient descent</strong></li>
</ul>
</li>
</ul>
<h3 id="multi-class-logistic-regression">Multi-class Logistic Regression</h3>
<ul>
<li><strong>&quot;One-vs-All&quot; Method</strong>: Creates $k$ binary classifiers for $k$ classes</li>
<li><strong>Loss Function</strong>:
<ul>
<li><strong>Softmax Loss</strong>: $p(y|x) = \frac{e^{\theta_k^T x}}{\sum_{k=1}^K e^{\theta_k^T x}}$</li>
<li>Optimized using <strong>MLE</strong></li>
</ul>
</li>
</ul>
<hr>
<h2 id="neural-network">Neural Network</h2>
<hr>
<h2 id="naive-bayes">Naive Bayes</h2>
<p><strong>Naive Bayes</strong> is a probabilistic classifier based on <strong>Bayes' theorem</strong>, assuming <strong>conditional independence</strong> among features</p>
<h3 id="key-concepts">Key Concepts</h3>
<ul>
<li>
<p><strong>Naive Bayes Assumption</strong>: Features $X_i$ are <strong>conditionally independent</strong> given $Y$:
<div>
$$
P(X|Y) = \prod_{i=1}^n P(X_i|Y)
$$
<div></p>
</li>
<li>
<p><strong>Goal</strong>: Compute $P(Y|X)$ using <strong>Bayes' theorem</strong>:
<div>
$$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$$
</div></p>
</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<ol>
<li>
<p><strong>Define Distributions</strong></p>
<ul>
<li>
<p>Specify the <strong>prior distribution</strong> $P(Y)$ for the class labels $Y$</p>
</li>
<li>
<p>Specify the <strong>likelihood distributions</strong> $P(X_i | Y=y)$ for each feature $X_i$, conditioned on the class $Y=y$</p>
</li>
<li>
<p>The distributions depend on the type of data:</p>
<ul>
<li><strong>Categorical Data</strong>: Use multinomial or Bernoulli distributions</li>
<li><strong>Continuous Data</strong>: Use Gaussian (Normal) distributions</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Estimate Parameters</strong></p>
<ul>
<li>Use the <strong>MLE</strong> or <strong>Maximum A Posteriori (MAP)</strong> to estimate the parameters of the distributions:
<ul>
<li>
<p>For $P(Y)$:
<div>
$$
P(Y=y) = \frac{\text{Number of instances with } Y=y}{\text{Total number of instances}}
$$
</div></p>
</li>
<li>
<p>For $P(X_i|Y=y)$:</p>
<ul>
<li>
<p>If the feature is categorical:
<div>
$$
P(X_i = x | Y = y) = \frac{\text{Count of } X_i = x \text{ and } Y = y + \alpha}{\text{Count of } Y = y + \alpha K}
$$
</div></p>
<p>where</p>
<ul>
<li>$\alpha$: Laplace smoothing factor</li>
<li>$K$: Number of unique categories for $X_i$</li>
</ul>
</li>
<li>
<p>If the feature is continuous:
<div>
$$
P(X_i | Y = y) = \frac{1}{\sqrt{2\pi\sigma_{y,i}^2}} \exp \left( -\frac{(X_i - \mu_{y,i})^2}{2\sigma_{y,i}^2} \right)
$$
</div></p>
<p>where</p>
<ul>
<li>$\mu_{y,i}$: Mean of $X_i$ in class $Y = y$</li>
<li>$\sigma_{y,i}^2$: Variance of $X_i$ in class $Y = y$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Predict for Input $X$</strong></p>
<ul>
<li>
<p>Compute the <strong>posterior probability</strong> for each class $y$ given the input $X = {X_1, X_2, \dots, X_n}$:
<div>
$$
P(Y=y|X) \propto P(Y=y) \prod_{i=1}^n P(X_i|Y=y)
$$
</div></p>
</li>
<li>
<p>To avoid numerical underflow, compute the <strong>log probabilities</strong>:
<div>
$$
\log P(Y=y|X) \propto \log P(Y=y) + \sum_{i=1}^n \log P(X_i|Y=y)
$$
</div></p>
</li>
</ul>
</li>
<li>
<p><strong>Select the Class</strong></p>
<ul>
<li>
<p>Identify the class with the highest <strong>posterior probability</strong>:
<div>
$$
y^* = \arg \max_{y \in \mathcal{Y}} P(Y=y|X)
$$
</div></p>
</li>
<li>
<p>If using <strong>log probabilities</strong>:
<div>
$$
y^* = \arg \max_{y \in \mathcal{Y}} \left( \log P(Y=y) + \sum_{i=1}^n \log P(X_i|Y=y) \right)
$$
</div></p>
</li>
</ul>
</li>
</ol>
<p>Example:</p>
<ul>
<li>Assume you want to classify an email as <strong>Spam</strong> or <strong>Not Spam</strong></li>
<li>$X$ represents the words in the email, and $Y$ represents the class (Spam or Not Spam)</li>
<li>Compute $P(\text{Spam})$, $P(\text{Not Spam})$, $P(\text{Word}|\text{Spam})$, and $P(\text{Word}|\text{Not Spam})$ based on training data</li>
<li>For a new email, calculate $P(\text{Spam|Email})$ and $P(\text{Not Spam|Email})$ and choose the class with the highest probability</li>
</ul>
<h3 id="laplace-smoothing">Laplace Smoothing</h3>
<p><strong>Laplace Smoothing</strong> addresses the issue of <strong>zero probabilities</strong> in Naive Bayes classification. If a feature value never occurs with a certain class in the training data, the estimated probability is zero, which can cause the entire probability product $P(X|Y)$ to become zero, making predictions impossible.</p>
<ul>
<li>
<p>For a categorical feature $j$ with $K$ unique values:
<div>
$$
\phi_{j,k|y=c} = P(x_j = v_k | y = c) = \frac{\sum_{i=1}^n \mathbb{I}(x_{ij} = v_k \text{ and } y_i = c) + 1}{\sum_{i=1}^n \mathbb{I}(y_i = c) + K}
$$
</div></p>
<ul>
<li>$\phi_{j,k|y=c}$: Smoothed conditional probability</li>
<li>$\mathbb{I}$: Indicator function (1 if true, 0 otherwise)</li>
<li>$K$: Number of unique values for feature $j$</li>
<li>+1: Adds a <strong>constant (pseudo-count)</strong> for each feature value, ensuring $\phi_{j,k|y=c} \neq 0$</li>
</ul>
</li>
</ul>
<p>Example:</p>
<ul>
<li>If a new word (&quot;neurips&quot;) appears in an email and was not seen during training, $P(\text{&quot;neurips&quot;}|Y) = 0$ without smoothing</li>
<li>Laplace smoothing ensures $P(\text{&quot;neurips&quot;}|Y) \neq 0$, allowing predictions to continue without failure</li>
</ul>
<hr>
<h2 id="support-vector-machine-svm">Support Vector Machine (SVM)</h2>
<img src="./res/svm.png" alt="" width="500">
<p><strong>SVM</strong> aims to find the <strong>hyperplane</strong> that <em>maximizes</em> the <strong>margin</strong> between two classes</p>
<h3 id="algorithm">Algorithm</h3>
<ul>
<li><strong>Hypothesis</strong>: Linear</li>
<li><strong>Loss</strong>: Hinge loss + Regularization term  <div>
  $$
  \text{minimize } \sum_{i=1}^m \max(1 - y^{(i)} \cdot \theta^T x^{(i)}, 0) + \frac{\lambda}{2} \|\theta\|^2
  $$
  </div>
</li>
<li><strong>Optimization</strong>: Quadratic programming</li>
</ul>
<h3 id="hyperparameters-in-svm">Hyperparameters in SVM</h3>
<ol>
<li>
<p><strong>Regularization Parameter ($C$)</strong>: Balances the trade-off between maximizing the margin and minimizing the classification error</p>
 <img src="./res/svm-c.png" alt="" width="500">
<ul>
<li>
<p><strong>High $C$</strong>:</p>
<ul>
<li>Penalizes misclassified points heavily</li>
<li>Results in a smaller margin, which fits the training data more closely (risk of overfitting)</li>
</ul>
</li>
<li>
<p><strong>Low $C$</strong>:</p>
<ul>
<li>Allows for more misclassified points</li>
<li>Results in a larger margin, which generalizes better to unseen data (less risk of overfitting)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Gamma</strong>: Defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'</p>
 <img src="./res/svm-gamma.png" alt="" width="500">
<ul>
<li>
<p><strong>Low Gamma</strong>:</p>
<ul>
<li>Each training example has a <strong>further</strong> influence</li>
<li>The model is more flexible and captures global patterns (risk of underfitting)</li>
</ul>
</li>
<li>
<p><strong>High Gamma</strong>:</p>
<ul>
<li>Each training example has a <strong>closer</strong> influence</li>
<li>The model fits closely to the training data and captures fine details (risk of overfitting)</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="comparison">Comparison</h3>
<ul>
<li>
<p><strong>SVM vs. Perceptron</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>SVM</th>
<th>Perceptron</th>
</tr>
</thead>
<tbody>
<tr>
<td>Focus</td>
<td>Maximizes the margin</td>
<td>Reduces error</td>
</tr>
<tr>
<td>Online Training</td>
<td>Not supported</td>
<td>Supported</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>SVM vs. Logistic Regression</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>SVM</th>
<th>Logistic Regression</th>
</tr>
</thead>
<tbody>
<tr>
<td>Margin Maximization</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Sensitivity to Outliers</td>
<td>Less sensitive</td>
<td>More sensitive</td>
</tr>
<tr>
<td>Output Type</td>
<td>Discrete (0 or 1)</td>
<td>Probabilities</td>
</tr>
<tr>
<td>Non-linear Decision Boundaries</td>
<td>Supported using kernel functions</td>
<td>Not supported</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3 id="advantages">Advantages</h3>
<ul>
<li>Effective in high-dimensional spaces</li>
<li>Can handle non-linear decision boundaries using kernel functions</li>
</ul>
<hr>
<h2 id="k-means-clustering">K-Means Clustering</h2>
<hr>
<h2 id="time-series-forecasting">Time Series Forecasting</h2>
<hr>
<p><a href="#top">Back to top</a></p>

</body>
</html>
